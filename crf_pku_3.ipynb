{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crf测试pku中文语料第3版\n",
    "\n",
    "改进：将训练集从[word,label]扩充成[word,cuttag,postag,label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pycrfsuite\n",
    "import jieba.posseg as jbpos\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentList = []\n",
    "f = open('./train_data', 'r', encoding='utf-8')\n",
    "pair = []\n",
    "for line in f:\n",
    "    line = line.encode('utf-8').decode('utf-8-sig').strip()  # 去掉首字前的\\ufeff\n",
    "    if line:\n",
    "        l = line.split(' ')\n",
    "        pair.append([l[0], l[1]])\n",
    "    else:\n",
    "        sentList.append(pair)\n",
    "        pair = []\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['去', 'N'], ['年', 'N'], ['，', 'N'], ['我', 'N'], ['们', 'N'], ['又', 'N'], ['被', 'N'], ['评', 'N'], ['为', 'N'], ['“', 'N'], ['北', 'B-LOC'], ['京', 'I-LOC'], ['市', 'I-LOC'], ['首', 'N'], ['届', 'N'], ['家', 'N'], ['庭', 'N'], ['藏', 'N'], ['书', 'N'], ['状', 'N'], ['元', 'N'], ['明', 'N'], ['星', 'N'], ['户', 'N'], ['”', 'N'], ['。', 'N']]\n"
     ]
    }
   ],
   "source": [
    "print(sentList[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(token):  # 把[[word, '', '', ''], [word, '', '', '']...]组合成'洗衣机，...'\n",
    "    sentence = ''\n",
    "    for t in token:\n",
    "        sentence += t[0]  # 取第一个字\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cut_and_seg(token):  # token格式为[[word, '', '', ''], [word, '', '', '']...]\n",
    "    # print(get_sentence(token))  # 海钓比赛地点在厦门与金门之间的海域。\n",
    "    wordlist = jbpos.cut(get_sentence(token))  # 海钓/nr 比赛/vn 地点/n 在/p 厦门/ns\n",
    "    res = list()\n",
    "    index = 0\n",
    "    for w in wordlist:\n",
    "        for i in range(len(w.word)):  # 考虑每个词语中的每个字\n",
    "            if len(w.word) == 1:  # 如果词语的长度为1，即只有一个字，标志为S\n",
    "                status = 'S'\n",
    "            elif i == 0:  # 如果词语的长度不为1，则第一个字，标志为B\n",
    "                status = 'B'\n",
    "            elif i == len(w.word) - 1:  # 如果词语的长度不为1，则最后一个字，标志为E\n",
    "                status = 'E'\n",
    "            else:  # 如果词语的长度不为1，则除了首/尾之外的字，标志为I\n",
    "                status = 'I'\n",
    "            token[index][1] = status  # 把每个字的S/B/I/E标志，赋值给token[index][1]\n",
    "            token[index][2] = w.flag  # 一个词语内的每个字的flag都是一样的\n",
    "            index += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    train_sents = []\n",
    "    test_sents = []\n",
    "    pairList = []\n",
    "    train = open('./train_data', 'r', encoding='utf-8')\n",
    "    test = open('./test_data', 'r', encoding='utf-8')\n",
    "    for line in train :\n",
    "        line = line.encode('utf-8').decode('utf-8-sig').strip()\n",
    "        if line:\n",
    "            l = line.split(' ')\n",
    "            word = l[0]\n",
    "            pos =l[1]\n",
    "            pairList.append([word,'','', pos])\n",
    "        else:\n",
    "            get_cut_and_seg(pairList)\n",
    "            train_sents.append(pairList)\n",
    "            pairList = []\n",
    "            \n",
    "    for line in test :\n",
    "        line = line.encode('utf-8').decode('utf-8-sig').strip()\n",
    "        if line:\n",
    "            l = line.split(' ')\n",
    "            word = l[0]\n",
    "            pos =l[1]\n",
    "            pairList.append([word,'','', pos])\n",
    "        else:\n",
    "            get_cut_and_seg(pairList)\n",
    "            test_sents.append(pairList)\n",
    "            pairList = [] \n",
    "            \n",
    "    train.close()\n",
    "    test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['“', 'S', 'x', 'N'], ['能', 'S', 'v', 'N'], ['选', 'B', 'v', 'N'], ['出', 'E', 'v', 'N'], ['一', 'B', 'm', 'N'], ['个', 'E', 'm', 'N'], ['好', 'S', 'a', 'N'], ['村', 'B', 'n', 'N'], ['长', 'E', 'n', 'N'], ['来', 'S', 'v', 'N'], ['，', 'S', 'x', 'N'], ['是', 'S', 'v', 'N'], ['我', 'B', 'r', 'N'], ['们', 'E', 'r', 'N'], ['老', 'B', 'n', 'N'], ['百', 'I', 'n', 'N'], ['姓', 'E', 'n', 'N'], ['的', 'S', 'uj', 'N'], ['福', 'B', 'n', 'N'], ['分', 'E', 'n', 'N'], ['！', 'S', 'x', 'N'], ['”', 'S', 'x', 'N'], ['记', 'B', 'n', 'N'], ['票', 'E', 'n', 'N'], ['板', 'S', 'ng', 'N'], ['上', 'S', 'f', 'N'], ['出', 'B', 'v', 'N'], ['现', 'E', 'v', 'N'], ['了', 'S', 'ul', 'N'], ['左', 'S', 'm', 'B-PER'], ['金', 'B', 'nz', 'I-PER'], ['文', 'E', 'nz', 'I-PER'], ['的', 'S', 'uj', 'N'], ['名', 'B', 'n', 'N'], ['字', 'E', 'n', 'N'], ['，', 'S', 'x', 'N'], ['而', 'B', 'c', 'N'], ['且', 'E', 'c', 'N'], ['票', 'B', 'n', 'N'], ['数', 'E', 'n', 'N'], ['扶', 'B', 'i', 'N'], ['摇', 'I', 'i', 'N'], ['直', 'I', 'i', 'N'], ['上', 'E', 'i', 'N'], ['，', 'S', 'x', 'N'], ['远', 'B', 'd', 'N'], ['远', 'E', 'd', 'N'], ['超', 'B', 'v', 'N'], ['过', 'E', 'v', 'N'], ['另', 'B', 'c', 'N'], ['外', 'E', 'c', 'N'], ['两', 'B', 'm', 'N'], ['名', 'E', 'm', 'N'], ['候', 'B', 'n', 'N'], ['选', 'I', 'n', 'N'], ['人', 'E', 'n', 'N'], ['。', 'S', 'x', 'N']]\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一', 'B', 'm', 'N'], ['个', 'E', 'm', 'N'], ['名', 'B', 'v', 'N'], ['叫', 'E', 'v', 'N'], ['左', 'S', 'm', 'B-PER'], ['金', 'B', 'nz', 'I-PER'], ['文', 'E', 'nz', 'I-PER'], ['的', 'S', 'uj', 'N'], ['村', 'B', 'n', 'N'], ['民', 'E', 'n', 'N'], ['竟', 'S', 'd', 'N'], ['自', 'B', 'r', 'N'], ['己', 'E', 'r', 'N'], ['投', 'S', 'v', 'N'], ['了', 'S', 'ul', 'N'], ['自', 'B', 'r', 'N'], ['己', 'E', 'r', 'N'], ['一', 'B', 'm', 'N'], ['票', 'E', 'm', 'N'], ['，', 'S', 'x', 'N'], ['而', 'B', 'c', 'N'], ['且', 'E', 'c', 'N'], ['主', 'B', 'b', 'N'], ['动', 'E', 'b', 'N'], ['向', 'S', 'p', 'N'], ['周', 'B', 'f', 'N'], ['围', 'E', 'f', 'N'], ['的', 'S', 'uj', 'N'], ['村', 'B', 'n', 'N'], ['民', 'E', 'n', 'N'], ['畅', 'B', 'v', 'N'], ['述', 'E', 'v', 'N'], ['他', 'S', 'r', 'N'], ['的', 'S', 'uj', 'N'], ['想', 'B', 'v', 'N'], ['法', 'E', 'v', 'N'], ['：', 'S', 'x', 'N'], ['“', 'S', 'x', 'N'], ['我', 'S', 'r', 'N'], ['竞', 'B', 'vn', 'N'], ['选', 'E', 'vn', 'N'], ['村', 'B', 'n', 'N'], ['长', 'E', 'n', 'N'], ['的', 'S', 'uj', 'N'], ['意', 'B', 'n', 'N'], ['思', 'E', 'n', 'N'], ['，', 'S', 'x', 'N'], ['大', 'B', 'n', 'N'], ['伙', 'E', 'n', 'N'], ['听', 'S', 'v', 'N'], ['一', 'B', 'm', 'N'], ['下', 'E', 'm', 'N'], ['，', 'S', 'x', 'N'], ['一', 'S', 'm', 'N'], ['不', 'B', 'v', 'N'], ['想', 'E', 'v', 'N'], ['争', 'B', 'i', 'N'], ['名', 'I', 'i', 'N'], ['夺', 'I', 'i', 'N'], ['利', 'E', 'i', 'N'], ['，', 'S', 'x', 'N'], ['就', 'S', 'd', 'N'], ['想', 'S', 'v', 'N'], ['为', 'S', 'p', 'N'], ['咱', 'S', 'r', 'N'], ['村', 'B', 'n', 'N'], ['民', 'E', 'n', 'N'], ['脚', 'B', 'i', 'N'], ['踏', 'I', 'i', 'N'], ['实', 'I', 'i', 'N'], ['地', 'E', 'i', 'N'], ['办', 'B', 'n', 'N'], ['实', 'I', 'n', 'N'], ['事', 'E', 'n', 'N'], ['，', 'S', 'x', 'N'], ['二', 'S', 'm', 'N'], ['不', 'S', 'd', 'N'], ['拉', 'B', 'n', 'N'], ['帮', 'I', 'n', 'N'], ['结', 'I', 'n', 'N'], ['派', 'E', 'n', 'N'], ['…', 'S', 'x', 'N'], ['…', 'S', 'x', 'N'], ['”', 'S', 'x', 'N'], ['片', 'B', 's', 'N'], ['中', 'E', 's', 'N'], ['，', 'S', 'x', 'N'], ['人', 'B', 'n', 'N'], ['们', 'E', 'n', 'N'], ['看', 'B', 'v', 'N'], ['到', 'E', 'v', 'N'], ['在', 'S', 'p', 'N'], ['民', 'B', 'n', 'N'], ['主', 'I', 'n', 'N'], ['选', 'I', 'n', 'N'], ['举', 'E', 'n', 'N'], ['之', 'B', 'f', 'N'], ['前', 'E', 'f', 'N'], ['，', 'S', 'x', 'N'], ['村', 'B', 'n', 'N'], ['民', 'E', 'n', 'N'], ['的', 'S', 'uj', 'N'], ['心', 'B', 'n', 'N'], ['态', 'E', 'n', 'N'], ['是', 'S', 'v', 'N'], ['漠', 'B', 'z', 'N'], ['然', 'E', 'z', 'N'], ['的', 'S', 'uj', 'N'], ['。', 'S', 'x', 'N']]\n"
     ]
    }
   ],
   "source": [
    "print(test_sents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    cuttag = sent[i][1]\n",
    "    postag = sent[i][2]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word=' + word,\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'cuttag=' + cuttag,\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i - 1][0]\n",
    "        postag1 = sent[i - 1][2]\n",
    "        cuttag1 = sent[i - 1][1]\n",
    "        features.extend([\n",
    "            '-1:word=' + word1,\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:cuttag=' + cuttag1,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i + 1][0]\n",
    "        postag1 = sent[i + 1][2]\n",
    "        cuttag1 = sent[i + 1][1]\n",
    "        features.extend([\n",
    "            '+1:word=' + word1,\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:cuttag=' + cuttag1,\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, cuttag, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, cuttag, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bias', 'word=藏', 'word.isdigit=False', 'postag=nz', 'cuttag=B', 'BOS', '+1:word=书', '+1:postag=nz', '+1:cuttag=E']\n"
     ]
    }
   ],
   "source": [
    "print(sent2features(train_sents[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n"
     ]
    }
   ],
   "source": [
    "print(sent2labels(train_sents[0])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]  # 不加[]是generator类型，不会输出。加了[]，可直接输出加了[]之后的结果。\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]  # 格式 [[[每个word的特征],[每个word的特征]],（到此处是一个s）,[(每个word的特征)],...]，\n",
    "y_test = [sent2labels(s) for s in test_sents] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6524\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "print(y_test[1111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xseq, yseq in zip(X_train, y_train): \n",
    "    trainer.append(xseq, yseq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer的设置参数\n",
    "trainer.set_params({\n",
    "    'c1':1.0,\n",
    "    'c2':1e-3,\n",
    "    'max_iterations':50,\n",
    "    'feature.possible_transitions':True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train('conll2002_3.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f4e9b598c88>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2002_3.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ture: N N N N N N N N N N N N N N N N N N N N N N N N N N N\n",
      "\n",
      "\n",
      "predicted: N N N N N N N N N N N N N N N N N N N N N N N N N N N\n"
     ]
    }
   ],
   "source": [
    "print('ture:', ' '.join(sent2labels(test_sents[17])))\n",
    "print('\\n')\n",
    "print('predicted:', ' '.join(tagger.tag(sent2features(test_sents[17]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_classification_report(y_true, y_pred):\n",
    "    '''\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    '''\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "    tagset = set(lb.classes_) - {'N'} \n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)} \n",
    "\n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels=[class_indices[cls] for cls in tagset],\n",
    "        target_names=tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.88      0.83      0.85      5391\n",
      "      I-LOC       0.83      0.82      0.83      7214\n",
      "      B-ORG       0.85      0.70      0.77      3082\n",
      "      I-ORG       0.86      0.75      0.80     12638\n",
      "      B-PER       0.91      0.81      0.86      2760\n",
      "      I-PER       0.91      0.86      0.88      5463\n",
      "\n",
      "avg / total       0.87      0.79      0.83     36548\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', ' '.join(tagger.tag('我爱北京天安门')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_words(sentence):\n",
    "    res = list()\n",
    "    for word in sentence:\n",
    "        res.append([word, '', '', ''])\n",
    "    get_cut_and_seg(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_encode(text):\n",
    "    if text.__contains__('PRO'):\n",
    "        return 'product_name'\n",
    "    elif text.__contains__('PER'):\n",
    "        return 'person_name'\n",
    "    elif text.__contains__('TIM'):\n",
    "        return 'time'\n",
    "    elif text.__contains__('ORG'):\n",
    "        return 'org_name'\n",
    "    elif text.__contains__('LOC'):\n",
    "        return 'location'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_boson_data_encode(text, tag):\n",
    "    res = \"\"\n",
    "    status = 0\n",
    "    for i in range(len(text)):\n",
    "        if status == 0 and tag[i] == 'N':  # 第一个字的标签\n",
    "            res += text[i]  # 不是要识别的字，加入到res中\n",
    "        elif status == 0 and tag[i] != 'N':  # 第一个字的标签是要识别的字\n",
    "            status = 1\n",
    "            res += \"{{\" + get_type_encode(tag[i]) + \":\" + text[i]\n",
    "        elif status == 1 and str(tag[i]).startswith('I'):  # 是要识别的字，而且是词中或词尾字\n",
    "            res += text[i]\n",
    "        elif status == 1:\n",
    "            res += \"}}\"\n",
    "            if tag[i] == 'N':\n",
    "                status = 0\n",
    "                res += text[i]\n",
    "            else:\n",
    "                status = 1\n",
    "                res += \"{{\" + get_type_encode(tag[i]) + \":\" + text[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text):\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open('conll2002_3.crfsuite')\n",
    "    sent = split_by_words(text)\n",
    "    tag_result = tagger.tag(sent2features(sent))\n",
    "    text_result = format_boson_data_encode(text, tag_result)\n",
    "    return text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 我站在{{location:北京}}的{{location:天安门}}门口\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', ner('我站在北京的天安门门口'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
